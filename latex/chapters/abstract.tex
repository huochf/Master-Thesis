\begin{abstract}[flattitle]
单目人和物体交互的三维重建任务是指从单视角的图片中重建出人和物体的三维信息，是计算机视觉和计算机图形学交叉领域的一个重要研究方向，在增强现实、物体操控学习、动画电影制作、机器人模仿学习、人体行为理解和具身智能等领域存在重要的应用。然而现存技术存在重建速度慢、对域外物体泛化能力弱、严重依赖三维标签监督等缺点，阻碍了其在实际场景中的应用。为了解决这些问题，本文围绕单目人和物体联合重建这个主题，在人和物体三维空间关系建模和预测、同类别物体的域外泛化、二维信息监督下的三维人-物空间关系先验学习三方面提出一系列创新型算法，有效地提升了单目重建精度和重建速度，提升了模型对同类别域外物体的泛化能力，缓解了模型严重依赖于三维标注的缺陷。具体来说，本文的主要研究内容和创新点如下：
\begin{enumerate}
    \item 针对人-物三维空间关系建模与预测，首先，提出了基于人-物偏移量的刻画方式，该刻画方式计算人和物体之间的偏移量并使用主成分分析的方法构建出人和物体空间关系的隐式空间。其次，提出使用叠层归一化流模型从输入图片中来预测基于偏移量表征的人-物空间关系后验分布。在后优化过程中通过约束人和物体的偏移量来限制人和物体之间的空间位置。在BEHAVE数据集和InterCap数据集上进行了大量实验，实验结果表明，相较于先前的方法，本文所提出的方法具有更高的重建精度和运行效率，尤其在应对人和物体严重遮挡的情况表现出更好的鲁棒性。
    \item 针对同类物体的域外泛化问题，提出了使用物体形状归一化函数将物体的形状统一映射到与交互相关的形状空间，在该形状空间下建立人和物体之间交互编码，并从数据集中学习基于该编码表征方式的人-物空间关系先验。在CHAIRS数据集上的实验结果表明该方法相比基准方法具有更好的泛化性。
    \item 针对模型严重依赖于三维标注的缺陷，提出了一种二维监督的算法，该算法从海量的二维图片中学习人和物体交互的先验知识。该方法使用最近邻聚类算法根据二维图片之间二维关键点的几何一致性进行聚类，使用归一化流模型从聚类的结果学习人和物体各种交互类型的摄像机视角分布以及在各个视角下人和物体二维关键点的排布。在后优化阶段引入先验损失和接触面损失来优化人和物体之间的相对位姿。为验证该模型在自然场景中的性能，构造出自然场景下人-物交互数据集。实验中，在室内BEHAVE数据集上在没有直接使用三维标注的前提下达到和之前三维监督方法近乎相媲美的性能，在室外WildHOI数据集上，和之前的方法相比对复杂场景和多样的交互类型上表现出更好的鲁棒性。
\end{enumerate}
\end{abstract}


\begin{abstract*}[flattitle]
Human-object interaction reconstruction from a single-view image aims at recovering the 3D information of the human and the objects from a single-view image. It is an important research direction in computer vision and computer graphics, with wide applications in augmented reality, object manipulation learning, animation film production, robot imitation learning, human behavior understanding, and embodied AI. However, existing methods suffer from slow reconstruction speed, weak generalization ability to novel objects, and heavy reliance on 3D label supervision, which hinders their practical applications. To address these issues, this work focuses on the topic of single-view human-object interaction reconstruction and proposes a series of innovative algorithms in three aspects: modeling and predicting the 3D spatial relationship between the human and the object, generalization to novel object within the same category, and prior learning from 3D human-object spatial relationships under 2D supervision. These algorithms effectively enhance both the accuracy of reconstruction and the speed of inference, improve the generalization ability of the model to novel objects within the same category, and alleviate the heavy reliance on 3D annotations.  Specifically, the main research contents and innovations of this paper are as follows:

\begin{enumerate}
    \item In order to model and predict the 3D spatial relationship between the human and objects, a novel representation and approach are proposed. The human-object offset is used to represent the spatial relationship between the human and the object. Based on this representation, a stacked normalizing flow model is proposed to predict the posterior distribution of the human-object spatial relation from the input images. During post-optimization, the relative spatial relation between the human and the object is constrained using the offset between the human and the object. Extensive experiments are conducted on the BEHAVE dataset and InterCap dataset, and the results show that the proposed method achieves higher reconstruction accuracy and computational efficiency compared with previous methods, especially when dealing with severe occlusions between the human and objects, demonstrating better robustness.
    \item In terms of generalization on novel objects within the same category, we proposed to use a shape normalization mapping to map the shapes of the object to an interaction-relevant shape space. In this shape space, we establish the interaction encoding between the human and the object and learn this prior knowledge of human-object spatial relationships based on this encoding representation from the dataset. Experimental results on the CHAIRS dataset show that this method has better generalization performance compared with the baseline method.
    \item As for the drawbacks of models heavily relying on the 3D annotations, a 2D-supervised algorithm is proposed, which learns the prior knowledge of human-object interactions from vast 2D images. The nearest neighbor clustering algorithm is deployed to cluster the images based on their geometric consistency of 2D keypoints. The normalizing flow is utilized to learn the camera viewpoint distribution as well as the arrangement of 2D keypoints of human and objects in each viewport. In the post-optimization stage, prior loss and contact loss are introduced to optimize the relative poses between the human and the object. To validate the performance of this method in the wild, a human-object interaction dataset named WildHOI is constructed. In experiments, on the indoor BEHAVE dataset, the model achieves almost comparable performance with previous 3D-supervised methods without directly using 3D annotations, and on the outdoor WildHOI dataset, it exhibits better robustness compared to PHOSA in complex scenes and diverse interaction types.
\end{enumerate}
\end{abstract*}